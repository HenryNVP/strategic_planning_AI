# Chat: Sync vs Streaming Mode

## ðŸ“Š Quick Comparison

| Aspect | Sync Mode | Streaming Mode |
|--------|-----------|----------------|
| **Endpoint** | `POST /chatbot/chat` | `POST /chatbot/chat/stream` |
| **Response Type** | Single JSON | Server-Sent Events (SSE) |
| **User Experience** | Wait â†’ Complete answer | Words appear immediately |
| **Use Case** | API integrations | Interactive chat UI |
| **Latency Feel** | Feels slower (wait for all) | Feels faster (see progress) |
| **Can Cancel?** | No | Yes (close stream) |
| **File Size** | Diagram: 21K | Diagram: 24K |

---

## ðŸ”„ Sync Mode (Standard Request/Response)

### **How it works:**
```
User: "What is strategic planning?"
         â†“
    [3-5 seconds...]
         â†“
Response: "Strategic planning is a process that organizations..."
```

### **Flow:**
1. User sends complete question
2. Backend processes (RAG + LLM)
3. **Wait for complete response**
4. Return entire answer at once
5. Display to user

### **Pros:**
- âœ… Simple to implement
- âœ… Works with any HTTP client
- âœ… Easy error handling
- âœ… Good for APIs & integrations

### **Cons:**
- âŒ User sees nothing until complete
- âŒ Feels slow for long answers
- âŒ Can't cancel mid-generation
- âŒ Poor UX for interactive chat

**Diagram:** `A4_03a_flow_chat_sync.puml`

---

## âš¡ Streaming Mode (Real-time Token Delivery)

### **How it works:**
```
User: "What is strategic planning?"
         â†“
Response appears word-by-word:
"Strategic" â†’ "planning" â†’ "is" â†’ "a" â†’ "process" â†’ ...
```

### **Flow:**
1. User sends complete question
2. Backend starts processing
3. **LLM generates tokens one-by-one**
4. Each token sent immediately via SSE
5. UI displays each word as it arrives
6. Final "done" signal sent

### **Pros:**
- âœ… Immediate visual feedback
- âœ… Feels much faster to user
- âœ… Can cancel mid-generation
- âœ… Better UX for long answers
- âœ… Like ChatGPT experience

### **Cons:**
- âŒ More complex to implement
- âŒ Requires SSE support
- âŒ Harder error handling
- âŒ Not ideal for non-UI clients

**Diagram:** `A4_03b_flow_chat_stream.puml`

---

## ðŸ”§ Technical Details

### **Sync Mode Response:**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "What is strategic planning?"
    },
    {
      "role": "assistant",
      "content": "Strategic planning is a process that organizations use to..."
    }
  ],
  "citations": ["doc_id_1", "doc_id_2"]
}
```

### **Streaming Mode Response (SSE):**
```
data: {"content": "Strategic", "done": false}

data: {"content": " planning", "done": false}

data: {"content": " is", "done": false}

...

data: {"content": "", "done": true}
```

---

## ðŸŽ¯ When to Use Each Mode

### **Use Sync Mode when:**
- Building API integrations
- Non-interactive applications
- Batch processing
- Simple clients without SSE support
- Testing & debugging

### **Use Streaming Mode when:**
- Building chat interfaces
- Interactive web applications
- Long responses expected
- User experience is priority
- Need cancellation capability

---

## ðŸ’» Implementation Notes

### **Backend (FastAPI):**
```python
# Sync mode
@router.post("/chatbot/chat")
async def chat(request: ChatRequest):
    response = await langgraph.invoke(...)
    return ChatResponse(messages=response)

# Streaming mode
@router.post("/chatbot/chat/stream")
async def chat_stream(request: ChatRequest):
    async for chunk in langgraph.astream(...):
        yield f"data: {json.dumps(chunk)}\n\n"
```

### **Frontend (JavaScript):**
```javascript
// Sync mode
const response = await fetch('/chatbot/chat', {...});
const data = await response.json();
displayMessage(data.messages);

// Streaming mode
const eventSource = new EventSource('/chatbot/chat/stream');
eventSource.onmessage = (event) => {
  const chunk = JSON.parse(event.data);
  if (!chunk.done) {
    appendToMessage(chunk.content);
  } else {
    eventSource.close();
  }
};
```

---

## ðŸ“š Related Diagrams

- **Sync Flow**: `A4_03a_flow_chat_sync.puml` (21K)
- **Streaming Flow**: `A4_03b_flow_chat_stream.puml` (24K)
- **History Management**: `A4_03c_flow_chat_history.puml` (19K)

---

**Last Updated**: 2025-10-29

